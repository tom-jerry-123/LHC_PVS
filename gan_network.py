"""
Class for generative adversarial network
Currently uses a GAN autoencoder
"""


import tensorflow as tf
import numpy as np
import keras
import json
import os


class GanAutoencoder:
    """
    Autoencoder with Discriminator. Code generated by ChatGPT
    """

    def __init__(self, input_dim, latent_dim):
        # Build autoencoder
        self._encoder = self._build_encoder(input_dim, latent_dim)
        self._decoder = self._build_decoder(latent_dim, output_dim=input_dim)
        autoencoder_output = self._decoder(self._encoder.output[0])
        self._autoencoder = tf.keras.models.Model(self._encoder.input, autoencoder_output, name="autoencoder")

        # Build and compile discriminator
        self._discriminator = self._build_discriminator(input_dim)
        self._discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                              loss='binary_crossentropy',
                              metrics=['accuracy'])

        # Compile combined model
        self._discriminator.trainable = False
        reconstructed = self._autoencoder(self._encoder.input)
        validity = self._discriminator(reconstructed)
        self._combined_model = tf.keras.models.Model(self._encoder.input, [reconstructed, validity])
        self._combined_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                               loss=['mse', 'binary_crossentropy'],
                               loss_weights=[0.5, 0.5])  # Adjust loss weights as necessary

        print(self._autoencoder.summary())

    @staticmethod
    def _build_encoder(input_dim, latent_dim):
        """
        Only use this to build the encoder network of autoencoder
        :return: tf.keras.Model
        """
        inputs = tf.keras.Input(shape=(input_dim,))
        x = tf.keras.layers.Flatten()(inputs)
        x = tf.keras.layers.Dense(32, activation='relu')(x)
        latent = tf.keras.layers.Dense(latent_dim, activation='relu')(x)
        return tf.keras.models.Model(inputs, latent, name='encoder')

    @staticmethod
    def _build_decoder(latent_dim, output_dim):
        inputs = tf.keras.Input(shape=(latent_dim,))
        x = tf.keras.layers.Dense(32, activation='relu')(inputs)
        outputs = keras.layers.Dense(input_shape=(output_dim,), activation='relu')(x)
        return tf.keras.models.Model(inputs, outputs, name='decoder')

    @staticmethod
    def _build_discriminator(input_dim):
        inputs = tf.keras.Input(shape=(input_dim,))
        x = tf.keras.layers.Dense(32, activation='relu')(inputs)
        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)
        return tf.keras.models.Model(inputs, x, name='discriminator')

    def train(self, X_train, epochs, batch_size=256, verbose=False):
        """
        Training loop for adversarial network
        :param X_train:
        :param epochs:
        :param batch_size:
        :param verbose:
        :return:
        """
        if batch_size < 16:
            batch_size = 16
            raise RuntimeWarning("Batch size < 16 not supported (batch size set to 16).")
        N_data_points = X_train.shape[0]

        for epoch in range(epochs):
            batch_idx_arrs = self._batch_partition(N_data_points, batch_size)

            for idxs in batch_idx_arrs:
                # 1. Train the discriminator. First set trainable to be true
                self._discriminator.trainable = True
                # Prepare samples
                real_samples = X_train[idxs]
                reconstructed_samples = self._autoencoder.predict(real_samples)
                all_samples = np.concatenate((real_samples, reconstructed_samples), axis=0)
                real = np.ones(idxs.shape[0])
                fake = np.zeros(idxs.shape[0])
                sample_labels = np.concatenate((real, fake))
                # Train, record loss
                d_loss = self._discriminator.train_on_batch(all_samples, sample_labels)
                # Remember to set the discriminator to be untrainable once we're done training it on this batch
                self._discriminator.trainable = False

                # 2. Train the combined model (autoencoder + adversarial)
                g_loss = self._combined_model.train_on_batch(real_samples, [real_samples, real])

            # Print the progress
            print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}%] [G loss: {g_loss}]")

    def _batch_partition(self, num_data, batch_size):
        idxs = np.arange(num_data)
        idxs = np.random.permutation(idxs)
        batch_idx_arrs = np.array_split(idxs, np.arange(batch_size, num_data, batch_size))
        if len(batch_idx_arrs[-1]) < 0.75 * batch_size:
            # cut last batch if it is too small to avoid training instability
            batch_idx_arrs = batch_idx_arrs[:-1]
        return batch_idx_arrs


"""
New attempt following examples on Keras and TF
"""
class GanNetwork(keras.Model):
    """
    ChatGPT did not give working model. New attempt following examples on Keras and TF
    """
    # Static fields
    bce_loss = keras.losses.BinaryCrossentropy(from_logits=False)
    mse_loss = keras.losses.MeanSquaredError()

    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self._autoencoder = GanNetwork.build_autoencoder(input_dim, latent_dim)
        self._discriminator = GanNetwork.build_discriminator(input_dim)
        self._d_optimizer = None
        self._a_optimizer = None
        # self.seed_generator = keras.random.SeedGenerator(1337)
        self._aut_loss_tracker = keras.metrics.Mean(name="autoencoder_loss")
        self._disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self._aut_loss_tracker, self._disc_loss_tracker]

    @staticmethod
    def build_autoencoder(input_dim, latent_dim):
        autoencoder = keras.Sequential(
            [
                keras.layers.InputLayer((input_dim,)),
                keras.layers.Dense(32, activation="relu"),
                keras.layers.Dense(latent_dim, activation="relu"),
                keras.layers.Dense(32, activation="relu"),
                keras.layers.Dense(input_dim, activation="relu"),
            ],
            name="autoencoder",
        )
        return autoencoder

    @staticmethod
    def build_discriminator(input_dim):
        discriminator = keras.Sequential(
            [
                keras.layers.InputLayer((input_dim,)),
                keras.layers.Dense(32, activation="relu"),
                keras.layers.Dense(1, activation="sigmoid"),
            ],
            name="discriminator",
        )
        return discriminator

    @staticmethod
    def discriminator_loss(real_outputs, fake_outputs):
        real_loss = GanNetwork.bce_loss(tf.ones_like(real_outputs), real_outputs)
        fake_loss = GanNetwork.bce_loss(tf.zeros_like(fake_outputs), fake_outputs)
        total_loss = (real_loss + fake_loss) * 0.5
        return total_loss

    @staticmethod
    def autoencoder_loss(inputs, reconstructions, fake_outputs):
        reco_loss = GanNetwork.mse_loss(inputs, reconstructions)
        critic_loss = GanNetwork.bce_loss(tf.ones_like(fake_outputs), fake_outputs)
        alpha = 0.5
        total_loss = alpha * reco_loss + (1 - alpha) * critic_loss
        return total_loss

    def compile(self, d_optimizer, a_optimizer):
        super().compile()
        self._d_optimizer = d_optimizer
        self._a_optimizer = a_optimizer

    def train_step(self, data):
        X_data, _trash = data  # throw away y_data, which is a duplicate of X_data as we are training an autoencoder

        # Train the discriminator
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            reconstructions = self._autoencoder(X_data, training=True)

            real_output = self._discriminator(X_data, training=True)
            fake_output = self._discriminator(reconstructions, training=True)

            gen_loss = self.autoencoder_loss(X_data, reconstructions, fake_output)
            disc_loss = self.discriminator_loss(real_output, fake_output)

        # Calculate gradients
        auto_grad = gen_tape.gradient(gen_loss, self._autoencoder.trainable_variables)
        disc_grad = disc_tape.gradient(disc_loss, self._discriminator.trainable_variables)

        # Apply gradients
        self._a_optimizer.apply_gradients(zip(auto_grad, self._autoencoder.trainable_variables))
        self._d_optimizer.apply_gradients(zip(disc_grad, self._discriminator.trainable_variables))

        # Update metrics
        self._aut_loss_tracker.update_state(gen_loss)
        self._disc_loss_tracker.update_state(disc_loss)

        return {
            "autoencoder_loss": self._aut_loss_tracker.result(),
            "discriminator_loss": self._disc_loss_tracker.result(),
        }
    
    def get_reconstruction_errors(self, input_data):
        """
        Calculates and returns the reconstruction errors of the autoencoder for given input data.
        
        :param input_data: Input data to be reconstructed
        :return: Reconstruction errors as a numpy array
        """
        reconstructions = self._autoencoder.predict(input_data)
        reconstruction_errors = np.sum((input_data - reconstructions) ** 2, axis=1)
        return reconstruction_errors
    
    def get_discriminator_predictions(self, input_data):
        """
        Calculates and returns the discriminator predictions for given input data.
        
        :param input_data: Input data to be evaluated by the discriminator
        :return: Discriminator predictions as a numpy array
        """
        return self._discriminator.predict(input_data)
    
    def save_model(self, filepath):
        """
        Save the GanNetwork model components to a directory.
        
        :param filepath: Directory path to save the model components
        """
        os.makedirs(filepath, exist_ok=True)
        
        # Save autoencoder
        self._autoencoder.save(os.path.join(filepath, 'autoencoder.keras'))
        
        # Save discriminator
        self._discriminator.save(os.path.join(filepath, 'discriminator.keras'))
        
        # Save optimizer configurations
        with open(os.path.join(filepath, 'optimizer_config.json'), 'w') as f:
            json.dump({
                'd_optimizer': {
                    'class_name': self._d_optimizer.__class__.__name__,
                    'config': self._d_optimizer.get_config()
                },
                'a_optimizer': {
                    'class_name': self._a_optimizer.__class__.__name__,
                    'config': self._a_optimizer.get_config()
                }
            }, f)
        
        print(f"Model components saved to {filepath}")
    
    @classmethod
    def load_model(cls, filepath):
        """
        Load a saved GanNetwork model from a directory.
        
        :param filepath: Directory path to the saved model components
        :return: Loaded GanNetwork instance
        """
        
        # Load autoencoder
        autoencoder = keras.models.load_model(filepath + "/autoencoder.keras")
        
        # Load discriminator
        discriminator = keras.models.load_model(filepath + '/discriminator.keras')
        
        # Create a new instance
        input_dim = autoencoder.input_shape[1]
        latent_dim = autoencoder.layers[2].output.shape[1]  # Assuming the latent layer is the 3rd layer
        gan = cls(input_dim, latent_dim)
        
        # Replace the components
        gan._autoencoder = autoencoder
        gan._discriminator = discriminator
        
        # Load and set optimizer configurations
        with open(filepath + '/optimizer_config.json', 'r') as f:
            optimizer_config = json.load(f)
        
        d_optimizer = keras.optimizers.get(optimizer_config['d_optimizer'])
        a_optimizer = keras.optimizers.get(optimizer_config['a_optimizer'])
        gan.compile(d_optimizer, a_optimizer)
        
        print(f"Model components loaded from {filepath}")
        return gan


